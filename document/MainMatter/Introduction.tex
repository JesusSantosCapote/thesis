\chapter*{Introducción}\label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introducción}

Desde finales del siglo XX, el crecimiento acelerado de Internet, la adopci\'on generalizada de computadoras 
personales y el desarrollo de las redes sociales ha provocado que una gran variedad de datos sean producidos a un ritmo 
sin precedentes y en grandes vol\'umenes. 
Este fen\'omeno, conocido como "Big Data" \cite{beyer2012importance}, ha impactado significativamente en diversas 
esferas de la actividad humana, facilitando el desarrollo de soluciones adaptables a las necesidades de diferentes 
campos de la ciencia y organizaciones industriales.

Al procesar estas grandes cantidades de datos, se genera información actualizada y relevante que puede ser empleada 
para generar nuevas hipótesis o descubrir tendencias, patrones y correlaciones ocultas en los datos. Con la aparición del 
Big Data, las técnicas de procesamiento y metodologías asociadas al análisis de datos han evolucionado para adaptarse a esta 
nueva realidad, ofreciendo formas más eficientes de manejar y analizar grandes cantidades de información.

El An\'alisis de Datos act\'ua como el proceso subyacente que energiza a otras tecnolog\'ias y procesos, nutriendolos
de los conocimientos derivados del an\'alisis. Tal es el 
caso de la Inteligencia de Negocios (Business Intelligence, BI), la cual es un proceso orientado a tecnolog\'ias (technology-driven) 
para analizar datos y proporcionar información accionable que ayuda a los ejecutivos, gerentes y otros usuarios corporativos 
a tomar decisiones comerciales informadas. La BI abarca una amplia gama de herramientas, aplicaciones y metodologías 
que permiten a las organizaciones recopilar datos de sistemas internos y fuentes externas, prepararlos para análisis, 
desarrollar y ejecutar consultas contra los datos y crear informes, paneles y visualizaciones de datos\cite{negash2004business}. 

Un elemento esencial de BI es el Procesamiento Analítico en Línea (Online Analytical Processing, OLAP), una tecnología que 
permite a los usuarios realizar análisis complejos en grandes cantidades de datos. OLAP permite la exploración de datos 
multidimensionales, proporcionando una forma de segmentar y desglosar los datos desde diferentes perspectivas. Admite 
operaciones analíticas avanzadas como el desglose (drill-down), la consolidación (roll-up) y el pivote, que ayudan a los 
usuarios a obtener información valiosa de sus datos.

Detrás del Procesamiento Analítico en Línea (OLAP) se encuentran los Almacenes de Datos (Data Warehouse), que son 
estructuras que permiten realizar de manera eficiente las operaciones OLAP. Su proceso de creaci\'on implica la 
recopilación, 
organización, integración y almacenamiento de datos provenientes de diversas fuentes en un repositorio centralizado. Este 
repositorio funciona como una base de datos consolidada y estructurada que brinda soporte a consultas y análisis eficientes. 
Además, proporciona una base sólida para actividades de OLAP y otras funciones de BI, al 
asegurar la consistencia de los datos, la integración y el almacenamiento de datos históricos.

\section{Motivaci\'on}

En Cuba, durante los \'ultimos años, se ha promovido la introducción de nuevas tecnologías en los procesos 
productivos como parte de la diversificación y modernización de la economía. Como resultado, tanto instituciones 
estatales como privadas, así como organizaciones, empresas y centros, han visto la necesidad de desarrollar un Almac\'en 
de Datos para aprovechar eficientemente los datos generados por su actividad económica.

La creación y mantenimiento de un Almacén de Datos implica la ejecución de los procesos ETL (Extracción, Transformación y 
Carga) para abastecerlo de datos. Estos procesos son fundamentales para garantizar la integridad y calidad de los datos 
almacenados. Sin embargo, la implementación manual de estos procesos puede presentar diversos desafíos
\cite{nwokeji2021systematic, dhaouadi2022data, kimball2004data}.

En primer lugar, la implementación manual de los procesos ETL puede resultar compleja debido a la necesidad 
de manejar múltiples fuentes de datos, transformarlos de acuerdo con las necesidades del almacén y cargarlos de manera 
eficiente. Para esto, se requiere un conocimiento profundo de las fuentes de datos y de las técnicas de 
transformación, captura y extracción.

La implementación manual de los procesos ETL es propensa a errores humanos. La manipulación de grandes 
volúmenes de datos aumenta el riesgo de errores, como omisiones, duplicaciones o inconsistencias en los datos cargados, lo 
cual compromete la calidad del almac\'en de datos. Asimismo, es bien sabido que la implementación de procesos ETL puede consumir una 
considerable cantidad de tiempo y recursos.

Existen herramientas que abordan estas limitaciones al ofrecer posibilidades para la automatización de procesos ETL. 
Sin embargo, en su mayoría, estas herramientas son servicios alojados en la nube de sus propietarios, lo cual 
dificulta su acceso y aprovechamiento por parte de las empresas cubanas. Esto se debe a las limitaciones de la 
infraestructura de comunicación en la isla y a la abundancia de bases de datos on premise en las empresas, lo cual 
dificulta el traspaso de los datos a la nube.

Frente a los desafíos inherentes de la programaci\'on manual de los procesos ETL, a la condiciones actuales
de la isla y al creciente auge de la automatización de procesos en el ámbito de la computación, resulta 
provechoso contemplar la creación de una herramienta para la automatización de los procesos ETL, que satisfaga 
las necesidades del empresariado nacional.

\section{Antecedentes}

En la facultad de Matemática y Computación de la Universidad de La Habana, la labor de investigación e innovación de 
los profesores del colectivo de Sistemas de Información se ha caracterizado no solo por su interrelación estrecha 
con el almacenamiento, el análisis y la obtención de información con vista a la toma de decisiones en diversos 
escenarios de aplicación, sino también por la creación de herramientas genéricas que contribuyan a agilizar los 
procesos de implementación y población de los repositorios de datos asociados a las soluciones analíticas de 
inteligencia de negocio. Al respecto, se encuentran los trabajos "Población genérica
de un Data Warehouse Empresarial"\cite{mijailmaster} y "Herramienta genérica para la población del 
Warehouse Informacional"\cite{lismaster}.

En el primero se propone un procedimiento para el diseño de procesos ETCL(Extracci\'on, Transformaci\'on, Depuraci\'on y Carga) 
compuesto por acciones, tareas y subprocesos. Las acciones engloban todas las posibles transformaciones que se pueden 
aplicar a los datos, así como los diversos tipos de extracción y carga. Por otro lado, las tareas constituyen un 
conjunto de acciones, mientras que los subprocesos se componen de un conjunto de tareas. De esta forma, un proceso 
ETCL puede ser modelado como un conjunto de subprocesos.

Además, brinda la implementación de una herramienta genérica para población del Data Warehouse Empresarial, basada en el 
diseño ETCL propuesto. La herramienta propuesta hace uso de la arquitectura pluggins\cite{noauthor_plug-architectures_nodate}, 
donde en su n\'ucleo se encuentra 
un agente ETCL encargado de ejecutar los subprocesos y las extensiones ser\'ian los disitintos tipos de acciones.

En el segundo se propone una formalización matemático computacional para el modelo de datos multidimensional. Además,
brinda la implementación de un ambiente de creación para el Warehouse Informacional que permite crear y administrar 
estructuras multidimensionales y gestinar los metadatos. En el ambiente se modela cada uno de los componentes del modelo 
dimensional como interfaces, que describen las funcionalidades y caracter\'isticas de cada uno, dejando el c\'omo a las 
implementaciones 
espec\'ificas para cada plataforma. Utiliza el patr\'on proxy o representante. Cada representante implementa 
las interfaces y sirven de intermediarios entre el ambiente y la tecnología que alojar\'a el Data Warehouse.

Esta investigaci\'on est\'a enmarcada en la tem\'atica de los Almacenes de Datos, inspir\'andose en los trabajos 
realizados por el grupo de investigación.


\section{Problem\'atica}

La generaci\'on autom\'atica de procesos ETL(Extracción, Transformación y Carga) es una tem\'atica amplia que a d\'ia de hoy no cuenta con una soluci\'on 
universalmente aceptada. Al realizar un acercamiento a las herramientas de generación automática de ETL más relevantes, podemos 
identificar un denominador común: la utilización de modelos gr\'aficos o programativos para la definición de 
escenarios ETL.

Siguiendo las pautas de la industria, en el presente trabajo se plantea la automatización de los procesos ETL necesarios 
para poblar un escenario analítico mediante la creaci\'on de un marco de trabajo que permita, a trav\'es de un lenguaje de dominio espec\'ifico
(DSL, Domain Specific Languaje), generar el c\'odigo ETL necesario para el proceso de población.

Una tabla de un Almacén de Datos puede contener atributos de m\'ultiples tablas de las fuentes de datos y 
atributos resultado de agregaciones o de la aplicaci\'on de otras funciones. Por tanto, la generaci\'on autom\'atica 
del proceso ETL encargado de poblar dicha tabla, pasa por la inferencia de los Joins necesarios para obtener los atributos 
que la componen. Luego, uno de los problemas primarios a resolver en 
el marco de la generaci\'on autom\'atica de procesos ETL es la inferencia de Joins, el cual en s\'i mismo es uno de 
los retos m\'as significativos de la disciplina y el cual ocup\'o la totalidad del tiempo de investigación e implementación del presente 
trabajo.

Finalmente, la presente investigación intentara dar respuesta a la siguiente interrogante: ¿será factible generar 
automáticamente la(s) consulta(s) asociada(s) a la ETL de interés con vistas a poblar el repositorio de datos 
correspondiente, apoy\'andose de un lenguaje de dominio espec\'ifico y la teor\'ia de grafos?

\section{Objetivos}

\subsection{Objetivo general}

Proponer y desarrollar un lenguaje de dominio específico para la definici\'on de escenarios analíticos, as\'i como un marco de trabajo que permita 
inferir los Joins necesarios para su población en el marco de un proceso ETL.

\subsection{Objetivos Espec\'ificos}

\begin{enumerate}
    \item Estudio de la bibliografía relacionada. 
    \item Estudio comparación de las herramientas existentes que permiten la automatización de procesos ETL.
    \item Diseño de un lenguaje de dominio específico para la definici\'on de escenarios analíticos.
    \item Implementación de un software que permita inferir y ejecutar los joins necesarios para la población del escenario diseñado.
    \item Validación de la solución mediante experimentación
\end{enumerate}

\section{Propuesta de soluci\'on}

Se propone utilizar un Lenguaje de Dominio Específico (DSL) como método de solución para la modelación conceptual de ETL. 
El DSL cuenta con estructuras gramaticales que permiten definir las dimensiones y los hechos del almacén de datos de 
destino.

El proceso comienza convirtiendo la fuente de datos en un grafo, donde las tablas se representan como nodos y las relaciones 
entre las tablas se representan como aristas. Este grafo, junto con las definiciones realizadas mediante el DSL, se utilizan 
como entrada para un algoritmo que inferiere los joins necesarios.

Una vez calculados los joins, se seleccionan los más adecuados y se generará el 
código SQL correspondiente al escenario ETL diseñado.

Finalmente, el sistema propuesto se encargará de ejecutar de manera planificada los códigos generados para mantener actualizado 
el almacén de datos. 

\section{Estructura del documento}

El resto del documento se ha estructurado en cuatro capítulos que abordan las distintas fases por las que transitó la 
presente investigación. En el cap\'itulo 1 se realiza un estudio sobre el marco te\'orico conceptual de los Sistemas de 
Inteligencia de Negocios (BI). En el cap\'itulo 2 se lleva a cabo un estudio de la actualidad de la generaci\'on autom\'atica 
de procesos ETL, exponiendo las especificidades de las principales herramientas del mercado que tratan de solventar esta 
problem\'atica. El cap\'itulo 3 constituye un acercamiento al diseño de la soluci\'on propuesta. En el capítulo 4 se 
detallan los aspectos técnicos de la implementación de un prototipo del sistema y se realiza un análisis de la validez de 
la solución implementada mediante el desarrollo de experimentos. A mode de descenlace, se presentan las conclusiones,
que recogen los resultados obtenidos de acuerdo al cumplimiento de los objetivos propuestos, así como las recomendaciones, 
donde se proponen un conjunto de líneas de investigación como parte de la continuación del presente trabajo. Finalmente, 
sepresentan las referencias bibliográficas que sustentan la base científica de la solución propuesta.