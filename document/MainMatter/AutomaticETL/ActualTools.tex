\section{Herramientas Actuales} \label{section:actual_tools}

\subsection{Amazon Glue}

AWS Glue es un servicio de ETL automático "serverless" disponible en AWS Cloud. Este servicio simplifica el proceso de 
extracción, transformación y carga de datos al eliminar la necesidad de configurar y administrar infraestructura de 
servidor. A continuación, se describen los pasos clave que conforman el funcionamiento de AWS Glue:

\subsubsection{Exploración de fuentes de datos:}
AWS Glue utiliza un componente llamado crawler para explorar las fuentes de datos 
especificadas por el usuario. El crawler analiza los datos y extrae los metadatos relevantes, como la estructura, el 
formato y la ubicación de los datos.

\subsubsection{Catálogo de Datos:}
Los metadatos extraídos por el crawler se almacenan en un repositorio central llamado Catálogo de 
Datos (Data Catalog). Este catálogo actúa como una base de conocimientos sobre los datos disponibles y puede ser 
consultado por los usuarios para obtener información sobre las fuentes de datos.

\subsubsection{Motor ETL:}
El Motor ETL (ETL Engine) utiliza los metadatos almacenados en el Catálogo de Datos para generar el código 
necesario para los procesos de ETL. Cuando el usuario especifica una base de datos de destino, el Motor ETL genera el 
código que integra los datos de las fuentes y los transforma en un formato compatible con el destino especificado.

\subsubsection{Schedulers:}
Los procesos de ETL generados por AWS Glue pueden ser activados manualmente o programados para ejecutarse en 
una frecuencia específica o cuando se lanze un determinado evento utilizando los Schedulers. Esto permite automatizar el 
flujo de trabajo de ETL y realizar actualizaciones periódicas de los datos.





\subsection{Oracle Data Integrator}

Oracle Data Integrator (ODI) es una herramienta de ELT autom\'atico, aunque también permite el desarrollo de escenarios 
ETL mediante su integración con Oracle Warehouse Builder, otro software del entorno de Oracle. Esto hace que sea una 
herramienta flexible y poderosa para el manejo, soluci\'on y despliegue de Almacenes de Datos. Posee una arquitectura 
cliente-servidor, con una aplicaci\'on de escritorio que se comunica con los sevidores de Oracle. Sus principales 
componentes son: 

\subsubsection{Repositorios:} 
Almacena información de configuración sobre la infraestructura de IT, metadatos de todas las aplicaciones, proyectos, 
escenarios y registros de ejecución. ODI cuenta con dos tipos de repositorios: un Repositorio Maestro (Master Repository) 
y varios Repositorios de Trabajo (Work Repositories). Los objetos creados mediante las interfaces de usuario son almacenados 
en ellos. El Repositorio Maestro almacena:

\begin{itemize}
    \item Información de seguridad, como usuarios y perfiles.
    \item Información topol\'ogica de los escenarios diseñados por los usuarios como esquemas, definici\'on de servidores, 
        contextos y lenguajes.
    \item Información sobre las versiones de los escenarios desarrollados.
\end{itemize}

Por otro lado, los repositorios de trabajo son los que realmente 
contienen los escenarios, incluido: 

\begin{itemize}
    \item La definición de esquemas, estructuras de las bases de datos involucradas y metadatos, 
        definiciones de campos y columnas, restricciones de calidad de los datos, referencias cruzadas y linaje de datos.
    \item Los proyectos con sus reglas comerciales definidas, paquetes instalados, procedimientos, 
        sistema de archivos, módulos de conocimiento, variables de entorno, etc.
    \item Registros de ejecución de escenarios e información de programación de tareas.
\end{itemize}

\subsubsection{Interfaces de Usuario:}

ODI Studio es la interfaz de usuario de Oracle Data Integrator que proporciona un entorno completo para trabajar con la 
herramienta. A través de ODI Studio, los usuarios pueden realizar diversas tareas, como consultar repositorios, desarrollar 
proyectos, programar tareas, operar y monitorear ejecuciones. La interfaz incluye navegadores que permiten visualizar y 
modificar los escenarios creados, así como el código generado automáticamente para su ejecución. Con ODI Studio, los 
usuarios tienen acceso a todas las funcionalidades necesarias para administrar y gestionar eficientemente los procesos de 
integración de datos. 

\subsubsection{Agente de Ejecución}

El agente de ejecución actúa como motor de ejecución para ODI. Es responsable de ejecutar interfaces de integración, 
transformaciones y otras tareas definidas en proyectos ODI. Se puede configurar para ejecutar las tareas al capturar 
cierto evento o programar para que ejecute sus tareas en determinados intervalos de tiempo. Soporta ejecución paralela
y distribuida as\'i como capacidades para el manejo y reporte de errores.

En el momento de diseño, los desarrolladores generan escenarios a partir de modelos gr\'aficos y reglas de negocio 
definidas mediante un lenguaje declarativo. Luego, el Agente de Ejecución recupera el código de estos escenarios 
del repositorio y lo ejecuta de forma aut\'onoma. 





\subsection{Google Dataflow}

Google Dataflow es un servicio "serverless" proporcionado por Google Cloud Platform para ejecutar "pipelines" de Apache Beam. 
Es capaz de capturar, procesar y analizar datos, en tiempo real o en bloques, provenientes de m\'ultiples fuentes.
Dataflow elimina la sobrecarga operativa de sus equipos de ingeniería de datos al automatizar el aprovisionamiento de la
infraestructura destino, ya sea un Almacen de Datos o un modelo de Machine Learning. Estas caracter\'isticas hacen de 
Dataflow una excelente opci\'on para la ejecución automática de ETL. Una soluci\'on de Google Dataflow consta de los 
siguientes elementos:

\subsubsection{Pipeline:}

El pipeline es el componente principal del software. Representa el flujo de trabajo de procesamiento de datos y est\'a 
compuesto por una serie de transformaciones aplicadas a los datos de entrada. Es definido por el usuario utilizando el 
Apache Beam SDK en Java o Python. Cada vez que una transformación, definida en el pipeline, es aplicada a los datos se crea 
un "PCollection", un conjunto de datos inmutable, para guardar los datos transformados. PCollection es el acr\'onimo de 
"Parallel Collection" (Colecci\'on 
Paralela). El motivo detrás de este nombre es que las PCollection están diseñadas para ser distribuidas en múltiples 
computadoras. La \'ultima PCollection es cargada en el almacenamiento de destino, pues los datos que contiene ya han pasado 
por todos los procesos de transformaci\'on. 

\subsubsection{Dataflow Workers:}

Una vez que el usuario ha creado un pipeline, Dataflow se encarga de su ejecución y despliegue, refiriéndose a esta 
acción como un "Dataflow Job" (Trabajo de Dataflow). Luego Dataflow asigna unas m\'aquinas virtuales llamadas "Workers" 
(Trabajadores) para ejecutar las transformaciones. El n\'umero de Trabajadores es manejado din\'amicamente por Dataflow 
y depender\'a de la complejidad del Trabajo. Los Trabajadores realizan todo el procesamiento computacional en la nube 
de Google.

\subsubsection{Paneles de Visualizaci\'on}

Dataflow provee al usuario de paneles con informaci\'on en tiempo real sobre distintas m\'etricas de los Trabajos as\'i 
como de alertas para la detecci\'on de fallos.





\subsection{Talend Open Studio}

Talend Open Studio(TOS) es otra herramienta de integración de datos perteneciente a la compañia Talend. A diferencia de las 
anteriores herramientas, TOS es un software libre y Open Source, lo que permite que pequeñas empresas puedan acceder a 
las bondades del procesamiento y an\'alisis de datos para mejorar su productividad. 

Su funcionamiento puede separarse en 
tres bloques: 

\subsubsection{Bloque Studio:}

Aqu\'i es donde los escenarios ETL son diseñados. Este bloque tiene tres subcomponentes: Interfaz de Usuario, Almacenamiento
y Generaci\'on de C\'odigo. Mediante la Interfaz de Usuario se pueden definir los modelos de negocios con sus escenarios ETL 
sin tener que escribir c\'odigo, basta con arrastrar y soltar componentes y luego conectarlos para definir una tarea de 
integración. Los modelos y las tareas creadas son guardadas en formato XML en el Almacenamiento junto con metadatos 
de las fuentes de datos. El subcomponente de Generaci\'on de C\'odigo se encarga de convertir las tareas en c\'odigo de 
Java. 

\subsubsection{Bloque de Ejecución:}

El servidor de la aplicaci\'on, llamado Talend Aplication 
Center (Centro de Aplicaci\'on de Talend), se encarga de desplegar y ejecutar, dentro de s\'i, los escenarios creados 
en el Bloque Studio, que es la aplicaci\'on cliente. El usuario puede desplegar uno o m\'as Jobs Servers 
(Servidores de Trabajo) dentro de su proyecto de sistema de informaci\'on para ejecutar las tareas del escenario diseñado 
siguiendo una programación basada en tiempos o en eventos. 

\subsubsection{Bloque de Repositorios:}

Dentro del Talend Aplication Center existen tres repositorios compartidos. Uno de ellos est\'a basado en un servidor SVN, 
el cual se encarga de almacenar metadatos de los proyectos, como los escenarios, los modelos de negocios, rutas, etc. 
El segundo es un servidor de base de datos y almacena metadatos de administraci\'on como cuentas de usuario y derechos de 
acceso. Los metadatos pueden ser compartidos por multiples usuarios y tareas. El tercer repositorio es un servidor 
Git para almacenar informaci\'on sobre las versiones de los escenarios diseñados.
