\section{Procesos ETL}\label{section:etl}

Los procesos ETL se encargan, a grandes rasgos, de convertir y unificar datos provenientes de diversas fuentes, generalmente 
con formatos distintos, en un \'unico repositorio de datos. Constituye un tipo especial de r\'eplica en la cual los datos 
capturados se modifican para obtener un escenario m\'as completo de una determinada actividad. ETL est\'a presente en la 
industria desde la década de 1970 y empez\'o a ganar popularidad con el auge de los almacenes de datos\cite{etl_vs_elt_amazon}.

\subsection{Objetivos de los procesos ETL}

Con la instrumentaci\'on de procesos ETL los desarrolladores buscan garantizar la calidad y confiabilidad de los datos para 
fines anal\'iticos y de toma de decisiones. Adem\'as, cumplen con otros objetivos clave. En una primera instancia, 
tienen el objetivo de conciliar datos de m\'ultiples fuentes, d\'igase bases de datos, hojas de c\'alculo, APIs, 
archivos planos y sistemas externos, en un formato unificado y estandarizado, que responda a la estructura del repositorio de destino,
sea un almac\'en de datos u otro tipo repositorio. 
La conciliación de los datos facilita 
los procesos de an\'alisis y de generación de informes de datos. En segundo lugar, con los procesos ETL se busca 
limpiar y transformar los datos, asegurando que sean precisos, completos y cumplan con las reglas y requisitos comerciales. 
Por \'ultimo, ETL permite la integración o conciliación de datos en tiempo real e históricos, lo cual brinda a las organizaciones una visión 
completa de sus datos a lo largo del tiempo, mejorando la obtenci\'on de conocimiento y la toma de decisiones.

\subsection{ETL vs ELT}

Extraer, Cargar y Transformar, ELT por sus siglas en ingl\'es es un proceso derivado de ETL solo que invierte las operaciones 
de carga y transformación. En ELT se cargan los datos en el sistema destino justo despu\'es de ser extra\'idos de la fuente 
o\'igen. El cómputo de las transformaciones de los datos extraídos, definidas por los desarrolladores, se realiza en el sistema de destino. 
La mayor parte de las 
transformaciones se realizan en la etapa de análisis y se cargan los datos en bruto mínimamente procesados en el 
almacenamiento de datos.

El uso m\'as tipico de ELT yace en el \'ambito del Big Data\cite{raunakjhawar_ETL_microsoft}. La adopción de la 
infraestructura en la nube, proporciona a los sistemas de destino la potencia de procesamiento y la capacidad de almacenamiento
necesaria para realizar transformaciones definidas sobre inmensas cantidades de datos.

Comparando ambos enfoques, con ELT se simplifica la arquitectura pues se elimina del proceso el motor de transformación. 
Tambi\'en, al escalar el almacenamientode datos de destino también se escala el rendimiento del proceso ELT pues es all\'i
donde se realizan las transformaciones. ELT omite el paso de copia presente en ETL que puede ser una operaci\'on muy costosa 
si el conjunto de datos es grande. Pero solo es efectivo usar este enfoque si el sistema destino es lo suficientemente
potente como para transformar los datos de manera eficiente.

Por otro lado, ETL es la mejor opci\'on para el tratamiento de datos estructurados\cite{etl_vs_elt_amazon}. Con 
ETL se transforma el formato de los datos, pero se mantiene su naturaleza estructurada. ETL es una tecnología madura 
con m\'as de 50 años de explotaci\'on, sus protocolos y buenas pr\'acticas son conocidos y bien documentados. Como principal 
desventaja le acompaña el hecho de que requiere m\'as definici\'on al principio, pues deben definirse los tipos de datos 
del destino, estructuras y relaciones.

\subsection{Operaciones de los Procesos ETL}

Como su nombre lo indica, las operaciones que conforman los Procesos ETL son:

\subsubsection{Extracci\'on}

La extracción o captura de los datos es el proceso que se encarga de interactuar con las fuentes para 
obtener una copia que puede contener todos sus datos, algunos de ellos o solo los cambios ocurridos. Esta operaci\'on 
siempre se realiza de acuerdo con la planificación de la réplica y no debe requerir intervención humana. 

El proceso de extracci\'on debe generar un impacto m\'inimo en el sistema or\'igen, si se excede su capacidad de respuesta 
es posible que colapse y no est\'e disponible para su uso. Por esta raz\'on, los grandes 
sistemas consumidores de datos programan sus actividades de extracci\'on para d\'ias u horarios donde su impacto sea 
m\'inimo. 

\subsubsection{Transformaci\'on}

Durante la fase de transformación, los datos extraídos de los sistemas fuente deben ser ajustados para estandarizar sus formatos, 
permitiendo así su integración coherente de acuerdo con la estructura y el diseño del sistema de destino. Estas modificaciones 
suelen diferir de la replicación convencional, requiriendo la ejecución de operaciones específicas combinadas. Entre las 
posibles transformaciones que se le pueden aplicar a los datos est\'an:

\begin{itemize}
    \item Seleccionar solo algunas columnas para ser cargadas.
    \item Traducir c\'odigos (por ejemplo, si la fuente almacena una M para Masculino y F para Femenino pero el destino 
        tiene que guardar 1 para Masculino y 2 para Femenino)
    \item Codificar valores, ejemplo de esto es convertir Principal en P o Secundario en S
    \item Eliminar datos duplicados.
    \item Revisi\'on de los formatos de los datos. Un caso com\'un de esto es la conversi\'on de unidades de medida 
        o la conversi\'on de los formatos de fecha/hora.
\end{itemize}

También se pueden realizar transformaciones m\'as avanzadas, que siguen las reglas del negocio para optimizar los datos y 
facilitar los análisis:

\begin{itemize}
    \item Aplicar directamente reglas comerciales a los datos, por ejemplo convertir los ingresos en ganancias restando los 
        gastos.
    \item Vincular datos de diferentes or\'igenes. Por ejemplo, calcular el costo total de compra de un producto 
        sumando el valor de compra de los diferentes proveedores y almacenando solo el total final en el sistema de destino.
    \item Cifrar datos confidenciales para cumplir con las leyes de datos o de privacidad antes de cargar la informaci\'on 
        en el sistema destino.
\end{itemize}


\subsubsection{Carga}

La fase de Carga es el momento en que los datos resultantes de la fase de Transformaci\'on son almacenados en sistema destino. 
En dependencia de los requisitos de cada organización, el proceso de carga abarca una variedad de acciones diferentes. 
En ocasiones se sobrescribe la información antigua de la bases de datos con nuevos datos. En cambio, los Almacenes de Datos 
conservan todos los datos con el objetivo de mantener un historial. La mayoría de las organizaciones que utilizan ETL, 
tienen este proceso automatizado, correctamente definido, continuo y por lotes\cite{ETL_amazon}. La carga de datos puede
ser de forma completa donde todos los datos de la fuente se transforman y se mueven al almacenamiento de datos, o bien 
puede ser de forma progresiva donde se carga la diferencia entre los sistemas de origen y destino a intervalos regulares.

\subsection{Herramientas para Procesos ETL}

Actualmente existen varias herramientas ETL en el mercado, cada una posee características propias y capacidades \'unicas. 
Entre las m\'as populares encontramos a Talend Data Fabric, Informatica PowerCenter, Fivetran, Stitch y Xplenty. Estas 
herramientas ofrecen gestión de datos basada en la nube, la integración basada en metadatos y soporte para varias bases 
de datos relacionales y no relacionales

Además de las opciones anteriores, existen varias opciones populares de c\'odigo abierto, como son Apache NiFi, AWS Glue 
e Informatica. Estas herramientas ofrecen casi todas las funcionalidades de sus contrapartes comerciales y a menudo 
son m\'as personalizables y flexibles.